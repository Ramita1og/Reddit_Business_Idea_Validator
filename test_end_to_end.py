"""
ç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬

æµ‹è¯•å®Œæ•´çš„ Reddit æ•°æ®æŠ“å–å’Œåˆ†ææµç¨‹
"""

import asyncio
import os
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
from agents.config import get_config
from agents.subagents.scraper_agent import ScraperAgent
from agents.context_store import ContextStore
from agents.skills.scraper_skills import (
    search_posts_skill,
    get_comments_skill,
    batch_get_comments_skill,
    batch_scrape_skill,
    batch_scrape_with_comments_skill
)
from mcp_servers.reddit_server import RedditMCPServer


async def test_search_posts():
    """æµ‹è¯•æœç´¢å¸–å­åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 1: æœç´¢å¸–å­")
    print("=" * 60)

    # åŠ è½½é…ç½®
    config_manager = get_config()
    context_store = ContextStore()
    
    # åˆ›å»º Reddit MCP æœåŠ¡å™¨
    reddit_server = RedditMCPServer(
        client_id=os.getenv("REDDIT_CLIENT_ID"),
        client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
        user_agent=os.getenv("REDDIT_USER_AGENT", "BusinessResearchAgent/1.0 by test_user")
    )
    await reddit_server.start()
    
    agent = ScraperAgent(
        config=config_manager,
        context_store=context_store,
        mcp_clients={"reddit": reddit_server}
    )

    # æœç´¢å¸–å­
    result = await search_posts_skill(
        agent,
        keyword="machine learning",
        sort="relevance",
        time_filter="all",
        limit=5
    )

    if result.get("success"):
        posts = result.get("posts", [])
        print(f"\nâœ… æœç´¢æˆåŠŸ! æ‰¾åˆ° {len(posts)} ä¸ªå¸–å­")
        
        for i, post in enumerate(posts[:3], 1):
            print(f"\n  [{i}] {post.get('title', 'N/A')}")
            print(f"      ID: {post.get('post_id')}")
            print(f"      å­ç‰ˆå—: {post.get('subreddit')}")
            print(f"      å¾—åˆ†: {post.get('score')}")
            print(f"      è¯„è®ºæ•°: {post.get('num_comments')}")
        
        await reddit_server.stop()
        return result
    else:
        print(f"\nâŒ æœç´¢å¤±è´¥: {result.get('error')}")
        await reddit_server.stop()
        return None


async def test_get_comments(post_id: str):
    """æµ‹è¯•è·å–è¯„è®ºåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: è·å–è¯„è®º")
    print("=" * 60)

    # åŠ è½½é…ç½®
    config_manager = get_config()
    context_store = ContextStore()
    
    # åˆ›å»º Reddit MCP æœåŠ¡å™¨
    reddit_server = RedditMCPServer(
        client_id=os.getenv("REDDIT_CLIENT_ID"),
        client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
        user_agent=os.getenv("REDDIT_USER_AGENT", "BusinessResearchAgent/1.0 by test_user")
    )
    await reddit_server.start()
    
    agent = ScraperAgent(
        config=config_manager,
        context_store=context_store,
        mcp_clients={"reddit": reddit_server}
    )

    # è·å–è¯„è®º
    result = await get_comments_skill(
        agent,
        post_id=post_id,
        limit=5
    )

    if result.get("success"):
        comments = result.get("comments", [])
        print(f"\nâœ… è·å–è¯„è®ºæˆåŠŸ! æ‰¾åˆ° {len(comments)} æ¡è¯„è®º")
        
        for i, comment in enumerate(comments[:3], 1):
            print(f"\n  [{i}] {comment.get('body', 'N/A')[:80]}...")
            print(f"      ä½œè€…: {comment.get('author')}")
            print(f"      å¾—åˆ†: {comment.get('score')}")
        
        await reddit_server.stop()
        return result
    else:
        print(f"\nâŒ è·å–è¯„è®ºå¤±è´¥: {result.get('error')}")
        await reddit_server.stop()
        return None


async def test_batch_get_comments(post_ids: list):
    """æµ‹è¯•æ‰¹é‡è·å–è¯„è®ºåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: æ‰¹é‡è·å–è¯„è®º")
    print("=" * 60)

    # åŠ è½½é…ç½®
    config_manager = get_config()
    context_store = ContextStore()
    
    # åˆ›å»º Reddit MCP æœåŠ¡å™¨
    reddit_server = RedditMCPServer(
        client_id=os.getenv("REDDIT_CLIENT_ID"),
        client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
        user_agent=os.getenv("REDDIT_USER_AGENT", "BusinessResearchAgent/1.0 by test_user")
    )
    await reddit_server.start()
    
    agent = ScraperAgent(
        config=config_manager,
        context_store=context_store,
        mcp_clients={"reddit": reddit_server}
    )

    # æ‰¹é‡è·å–è¯„è®º
    result = await batch_get_comments_skill(
        agent,
        post_ids=post_ids,
        comments_per_post=3
    )

    if result.get("success"):
        results = result.get("results", {})
        total_comments = result.get("total_comments", 0)
        print(f"\nâœ… æ‰¹é‡è·å–è¯„è®ºæˆåŠŸ!")
        print(f"  å¤„ç†çš„å¸–å­æ•°: {len(results)}")
        print(f"  æ€»è¯„è®ºæ•°: {total_comments}")
        
        for post_id, comments in results.items():
            print(f"\n  å¸–å­ {post_id}: {len(comments)} æ¡è¯„è®º")
        
        await reddit_server.stop()
        return result
    else:
        print(f"\nâŒ æ‰¹é‡è·å–è¯„è®ºå¤±è´¥: {result.get('error')}")
        await reddit_server.stop()
        return None


async def test_batch_scrape():
    """æµ‹è¯•æ‰¹é‡æŠ“å–åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: æ‰¹é‡æŠ“å–")
    print("=" * 60)

    # åŠ è½½é…ç½®
    config_manager = get_config()
    context_store = ContextStore()
    
    # åˆ›å»º Reddit MCP æœåŠ¡å™¨
    reddit_server = RedditMCPServer(
        client_id=os.getenv("REDDIT_CLIENT_ID"),
        client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
        user_agent=os.getenv("REDDIT_USER_AGENT", "BusinessResearchAgent/1.0 by test_user")
    )
    await reddit_server.start()
    
    agent = ScraperAgent(
        config=config_manager,
        context_store=context_store,
        mcp_clients={"reddit": reddit_server}
    )

    # æ‰¹é‡æŠ“å–
    result = await batch_scrape_skill(
        agent,
        keywords=["AI", "machine learning"],
        max_posts=3,
        comments_per_post=5
    )

    if result.get("success"):
        posts = result.get("posts", [])
        comments = result.get("comments", {})
        total_posts = result.get("total_posts", 0)
        total_comments = result.get("total_comments", 0)
        
        print(f"\nâœ… æ‰¹é‡æŠ“å–æˆåŠŸ!")
        print(f"  æ€»å¸–å­æ•°: {total_posts}")
        print(f"  æ€»è¯„è®ºæ•°: {total_comments}")
        print(f"  æ‰§è¡Œæ—¶é—´: {result.get('execution_time', 0):.2f}ç§’")
        
        keyword_results = result.get("keyword_results", {})
        for keyword, stats in keyword_results.items():
            print(f"\n  å…³é”®è¯ '{keyword}':")
            print(f"    å¸–å­æ•°: {stats.get('posts_count', 0)}")
            print(f"    è¯„è®ºæ•°: {stats.get('comments_count', 0)}")
        
        await reddit_server.stop()
        return result
    else:
        print(f"\nâŒ æ‰¹é‡æŠ“å–å¤±è´¥: {result.get('error')}")
        await reddit_server.stop()
        return None


async def test_batch_scrape_with_comments():
    """æµ‹è¯•æ‰¹é‡æŠ“å–å¹¶åˆå¹¶è¯„è®ºåŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: æ‰¹é‡æŠ“å–å¹¶åˆå¹¶è¯„è®º")
    print("=" * 60)

    # åŠ è½½é…ç½®
    config_manager = get_config()
    context_store = ContextStore()
    
    # åˆ›å»º Reddit MCP æœåŠ¡å™¨
    reddit_server = RedditMCPServer(
        client_id=os.getenv("REDDIT_CLIENT_ID"),
        client_secret=os.getenv("REDDIT_CLIENT_SECRET"),
        user_agent=os.getenv("REDDIT_USER_AGENT", "BusinessResearchAgent/1.0 by test_user")
    )
    await reddit_server.start()
    
    agent = ScraperAgent(
        config=config_manager,
        context_store=context_store,
        mcp_clients={"reddit": reddit_server}
    )

    # æ‰¹é‡æŠ“å–å¹¶åˆå¹¶è¯„è®º
    result = await batch_scrape_with_comments_skill(
        agent,
        keywords=["AI"],
        max_posts=3,
        comments_per_post=5
    )

    if result.get("success"):
        posts_with_comments = result.get("posts_with_comments", [])
        metadata = result.get("metadata", {})
        
        print(f"\nâœ… æ‰¹é‡æŠ“å–å¹¶åˆå¹¶è¯„è®ºæˆåŠŸ!")
        print(f"  æ€»å¸–å­æ•°: {metadata.get('total_posts', 0)}")
        print(f"  æœ‰è¯„è®ºçš„å¸–å­: {metadata.get('posts_with_comments', 0)}")
        print(f"  æ— è¯„è®ºçš„å¸–å­: {metadata.get('posts_without_comments', 0)}")
        print(f"  æ€»è¯„è®ºæ•°: {metadata.get('total_comments', 0)}")
        
        for i, post in enumerate(posts_with_comments[:2], 1):
            print(f"\n  [{i}] {post.get('title', 'N/A')[:50]}...")
            print(f"      å¸–å­ID: {post.get('post_id')}")
            print(f"      è¯„è®ºæ•°: {len(post.get('comments_data', []))}")
        
        await reddit_server.stop()
        return result
    else:
        print(f"\nâŒ æ‰¹é‡æŠ“å–å¹¶åˆå¹¶è¯„è®ºå¤±è´¥: {result.get('error')}")
        await reddit_server.stop()
        return None


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("Reddit ä¸šåŠ¡è°ƒç ”ç³»ç»Ÿ - ç«¯åˆ°ç«¯æµ‹è¯•")
    print("=" * 60)

    load_dotenv()

    # æ£€æŸ¥ Reddit å‡­è¯
    client_id = os.getenv("REDDIT_CLIENT_ID")
    client_secret = os.getenv("REDDIT_CLIENT_SECRET")

    if not client_id or not client_secret:
        print("\nâŒ é”™è¯¯: ç¼ºå°‘ Reddit å‡­è¯")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® REDDIT_CLIENT_ID å’Œ REDDIT_CLIENT_SECRET")
        return False

    if client_id.startswith("your_reddit") or client_secret.startswith("your_reddit"):
        print("\nâŒ é”™è¯¯: æ£€æµ‹åˆ°å ä½ç¬¦å‡­è¯")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®çœŸå®çš„ Reddit API å‡­è¯")
        return False

    all_passed = True

    # æµ‹è¯• 1: æœç´¢å¸–å­
    search_result = await test_search_posts()
    if not search_result:
        all_passed = False

    # æµ‹è¯• 2: è·å–è¯„è®º
    if search_result and search_result.get("posts"):
        post_id = search_result["posts"][0].get("post_id")
        comments_result = await test_get_comments(post_id)
        if not comments_result:
            all_passed = False

    # æµ‹è¯• 3: æ‰¹é‡è·å–è¯„è®º
    if search_result and len(search_result.get("posts", [])) >= 2:
        post_ids = [p.get("post_id") for p in search_result["posts"][:2]]
        batch_comments_result = await test_batch_get_comments(post_ids)
        if not batch_comments_result:
            all_passed = False

    # æµ‹è¯• 4: æ‰¹é‡æŠ“å–
    batch_scrape_result = await test_batch_scrape()
    if not batch_scrape_result:
        all_passed = False

    # æµ‹è¯• 5: æ‰¹é‡æŠ“å–å¹¶åˆå¹¶è¯„è®º
    batch_scrape_with_comments_result = await test_batch_scrape_with_comments()
    if not batch_scrape_with_comments_result:
        all_passed = False

    # æ€»ç»“
    print("\n" + "=" * 60)
    if all_passed:
        print("âœ… æ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡!")
        print("=" * 60)
        print("\nğŸ‰ Reddit ä¸šåŠ¡è°ƒç ”ç³»ç»Ÿå·²æˆåŠŸéƒ¨ç½²å¹¶æµ‹è¯•!")
        print("ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡Œ Reddit æ•°æ®æŠ“å–å’Œåˆ†æã€‚")
        return True
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        print("=" * 60)
        print("\nè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶ä¿®å¤é—®é¢˜ã€‚")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
